---
title: "Untitled"
format: html
---

```{python}
import numpy as np
import pandas as pd
import openai
import configparser
import re 
import time
from  scipy.special import expit, logit

# import oai_helpers as oh
from oai_helpers import *

config = configparser.ConfigParser()
config.read("config.ini")
api_key = config.get('Keys','openai_api_key')

openai.api_key = api_key

```




## Relational Match-To-Sample

I'm going to try to use GPT-3 to learn relational match to sample. I'm going to start with the simplest case of matching two-digit numbers to two-digit numbers. Then I'll move on to matching arbitrary token sequences of varying length (again using two-digit numbers as the tokens)

### Creating RMTS dataset for testing

```{python}

### Make data
rng = np.random.default_rng(123)
N = 100

def make_trial_data(N, rng):
    df = pd.DataFrame({"type": ["same"]*int(N/2) + ["different"]*int(N/2), "p1": rng.integers(1, 51, size=N)})
    df["p2"] = df.apply(lambda x: x["p1"] if x["type"] == "same" else rng.integers(1, 51, size=1)[0], axis=1)
    df["m1"] = rng.integers(1, 51, size=N)
    df["correct"] = df.apply(lambda x: x["m1"] if x["type"] == "same" else rng.integers(1, 51, size=1)[0], axis=1)
    df["foil"] = df.apply(lambda x: x["m1"] if x["type"] == "different" else rng.choice([x["p1"], x["p2"]]), axis=1)

    ## now make sure correct and foil are not the same by changing the foil if needed
    df["foil"] = df.apply(lambda x: x["p1"] if x["foil"] == x["correct"] else x["foil"], axis=1)

    ## now add a new column prompts, randomizing whether "correct" or "foil" is the first or second option

    df["option1"] = df.apply(lambda x: x["correct"] if rng.choice([True, False]) else x["foil"], axis=1)
    df["option2"] = df.apply(lambda x: x["correct"] if x["option1"] == x["foil"] else x["foil"], axis=1)

    return(df)


df = make_trial_data(N, rng)
```


```{python}
## make data with characters instead of numbers

# df = pd.DataFrame({"type": ["same"]*50 + ["different"]*50, "p1": rng.choice(list("abcdefghijklmnopqrstuvwxyz"), size=N)})
# df["p2"] = df.apply(lambda x: x["p1"] if x["type"] == "same" else rng.choice(list("abcdefghijklmnopqrstuvwxyz"), size=1)[0], axis=1)
# df["m1"] = rng.choice(list("abcdefghijklmnopqrstuvwxyz"), size=N)
# df["correct"] = df.apply(lambda x: x["m1"] if x["type"] == "same" else rng.choice(list("abcdefghijklmnopqrstuvwxyz"), size=1)[0], axis=1)
# df["foil"] = df.apply(lambda x: x["m1"] if x["type"] == "different" else rng.choice([x["p1"], x["p2"]]), axis=1)


# ## define a function to return a new random character until the character returned does not match the input
# def get_new_char(x):
#     new_char = rng.choice(list("abcdefghijklmnopqrstuvwxyz"))
#     while new_char == x:
#         new_char = rng.choice(list("abcdefghijklmnopqrstuvwxyz"))
#     return(new_char)

# ## now make sure correct and foil are not the same by changing the foil if needed
# df["foil"] = df.apply(lambda x: get_new_char(x["correct"]) if x["foil"] == x["correct"] else x["foil"], axis=1)

# ## now add a new column prompts, randomizing whether "correct" or "foil" is the first or second option

# df["option1"] = df.apply(lambda x: x["correct"] if rng.choice([True, False]) else x["foil"], axis=1)
# df["option2"] = df.apply(lambda x: x["correct"] if x["option1"] == x["foil"] else x["foil"], axis=1)
# df["prompt"] = df.apply(lambda x: make_rmts_prompt(x["p1"], x["p2"], x["m1"], x["option1"], x["option2"]), axis=1)

# df["answer"] = df.apply(lambda x: "A" if x["option1"] == x["correct"] else "B", axis=1)
```


# Testing GPT-3

## Cloze vs multiple choice

This [paper](https://arxiv.org/pdf/2210.12353.pdf) proposes prompting with a standard multiple-choice style prompt and looking for the token GPT-3 responds with. The authors claim that this is a better way to evaluate GPT-3 than the cloze-style prompt.

A cloze-style prompt appends or embeds the answer and then examines the probability of the relevant tokens. There are various approaches to normalizing this. One that seems relevant would be the unconditional probability of the answer, just as a token by itself, or as in some other more minimal context, e.g. in the presence of "Answer: ". In my case it would be in the presence of "[" or maybe just the second row?

It seems worth exploring the cloze-style approach as well, since the zero-shot performance otherwise appears to be chance.

```{python}

# def enclose_item(item): # this one worked a bit better zero shot
#     return("{" + str(item) + "}")


# def make_rmts_cloze_prompt(p1, p2, m1, completed): # this one worked a bit better zero shot
#     string = enclose_item(p1) + "\n.\n" + enclose_item(p2) + "\n---\n" +  enclose_item(m1) + "\n.\n" + enclose_item(completed)

#     return(string)

# def make_normalization_prompt(m1, completed): # this one worked a bit better zero shot
#     string = "\n"+ enclose_item(m1) + " " + enclose_item(completed)

#     return(string)    



def make_rmts_cloze_prompt(p1, p2, m1, completed):
    string = str(p1) + "\n.\n" + str(p2) + "\n---\n" + str(m1) + "\n.\n" + str(completed)

    return(string)    

def make_normalization_prompt(m1, completed):
    string = "\n---\n"+ str(m1) + "\n.\n" + str(completed)

    return(string)    

def make_cloze_prompts(df, option_name="correct"):
    prompts = [make_rmts_cloze_prompt(i[0], i[1],i[2], i[3]) for i in zip(df["p1"], df["p2"], df["m1"], df[option_name])]

    return(prompts)

def make_cloze_norm_prompts(df, option_name="correct"):
    prompts = [make_normalization_prompt(i[0], i[1]) for i in zip(df["m1"], df[option_name])]

    return(prompts)


def compute_cloze_prob(df, option_name="correct", prompt="", ind = -2):
    cloze_prompts = make_cloze_prompts(df, option_name)
    cloze_norm_prompts = make_cloze_norm_prompts(df, option_name)

    cloze_logprobs = gpt_token_probs(cloze_prompts, prompt=prompt)
    cloze_norm_logprobs = gpt_token_probs(cloze_norm_prompts) # may help to add prompt here too, unclear

    option_logprobs = np.array([r[ind] for r in cloze_logprobs])
    norm_logprobs = np.array([r[ind] for r in cloze_norm_logprobs])

    # return(option_logprobs)

    return(option_logprobs - norm_logprobs)
    # return(np.log(expit(option_logprobs)) - np.log(expit(norm_logprobs)))

```

```{python}

x = compute_cloze_prob(df, "correct", ind=-1)
y = compute_cloze_prob(df, "foil", ind=-1)

```


```{python}
df["gpt_correct_zeroshot"] = x > y
np.mean(df["gpt_correct_zeroshot"] )
```

Well it clearly matters a lot how you normalize the cloze probabilities. I don't think that the Brown et al approach makes much sense as a "normalization" -- the result is no longer a probability! What is working best so far is computing something like the "evidence" -- the difference in the logits for the full context and the minimal context. Without any normalization it tends to want to use the "same" response for everything, so it does nearly perfectly for the "same" items but does worse than chance for the "different" items. 


```{python}
df_examples = make_trial_data(32, np.random.default_rng(256))
examples_prompts = make_cloze_prompts(df_examples.sample(frac=1)[0:32], "correct")
```

```{python}

x = compute_cloze_prob(df, "correct", prompt = "\n\n".join(examples_prompts) + "\n\n", ind=-1) 
y = compute_cloze_prob(df, "foil", prompt = "\n\n".join(examples_prompts) + "\n\n", ind=-1)

```


```{python}
df["gpt_correct_fewshot"] = x > y
np.mean(df["gpt_correct_fewshot"] )
```

Ok so multi-shot improves performance! Up from 74% to 82% for 16-shot. And up to 95% for 32-shot!

__comment__: It is somewhat sensitive to the format that things are presented in. The `[a][b]\n[c][d]` approach seems to be best for zero-shot. But with some examples my simpler format also seems to work. And this will work for the matrices too (at least in principle it should).

## Multiple-choice answers

```{python}
# q_prompt = 'Which would complete the pattern in the place of the question mark?'
q_prompt = ""

def enclose_item(item):
    return("[" + str(item) + "]")

# def make_rmts_prompt(p1, p2, m1, option1, option2):
#     string = enclose_item(p1) + "\n" + enclose_item(p1) + "\n" + enclose_item(p2) + "\n---\n" +  enclose_item(m1) + "\n" +  enclose_item(m1) + "\n" +  enclose_item("?") + "\n\n" + q_prompt + "\n" + "a: " + str(option1) + "\nb: " + str(option2) + "\n\nAnswer:"

#     return(string)

def make_mc_prompt(p1, p2, m1, option1, option2):
    string = enclose_item(p1) + " " + enclose_item(p2) + "\n" +  enclose_item(m1) + " " +  "?" + "\n" + q_prompt + "\n" + "a. " + enclose_item(option1) + "\nb. " + enclose_item(option2) + "\n\nAnswer:"

    return(string)

```

```{python}
df["prompt"] = df.apply(lambda x: make_mc_prompt(x["p1"], x["p2"], x["m1"], x["option1"], x["option2"]), axis=1)
## add a new column "answer" which is "A" if option1 is correct, "B" if option2 is correct
df["answer"] = df.apply(lambda x: "A" if x["option1"] == x["correct"] else "B", axis=1)

prompts = df["prompt"].tolist()

print(prompts[0])
```


```{python}
responses = gpt_complete(prompts, sleep=0, temperature = 0, max_tokens = 1, stop = [":", "]"])
```

```{python}
df["gpt_answer"] = responses
df["gpt_acc"] = df.apply(lambda x: x["answer"].lower() == x["gpt_answer"] or str(x["correct"])==x["gpt_answer"], axis=1)
print("acc:", df["gpt_acc"].mean())
```

Looks like it doesn't do well! Doesn't seem to do well with zero-shot relational match to sample task. Surprising because with my small anecdotal sample it seemed like it was doing very well


```{python}
example_same1 = make_rmts_prompt("2","2","3", "3", "2") + " a\n\n"
example_same2 = make_rmts_prompt("45","45","29", "45", "29") + " b\n\n"

example_diff1 = make_rmts_prompt("17","12","12", "12", "5") + " b\n\n"
example_diff2 = make_rmts_prompt("5","7","7", "2", "7") + " a\n\n"
```

```{python}

fewshot_prompt = example_diff1 + example_same1 + example_diff2 + example_same2

responses_1shot = gpt_complete(prompts, prompt = fewshot_prompt, sleep=0.1, temperature = 0, max_tokens = 12, stop = ["<", "\n"])

```

```{python}

def clean_response(x):
    return(x[0])


df["gpt_answer_1shot"] = [clean_response(r) for r in responses_1shot]
df["gpt_acc_1shot"] = df.apply(lambda x: x["answer"].lower() == x["gpt_answer_1shot"] or str(x["correct"])==x["gpt_answer_1shot"], axis=1)
print("acc (1shot):", df["gpt_acc_1shot"].mean())
```

Gets to similar performance as the best (still hacky) cloze-style approach with fewshot examples. Possibly it's partially struggling with the matching of answers to the options in the prompt, which the fewshot examples help to clear up. 


-------

# Matrix things below

Not relevant yet ...

```{python}
import numpy as np

def make_np_matrix_progression(svals, summands):
    # svals: numpy vector of starting values
    # summands: numpy vector of integer summands

    dims = (len(svals), len(svals))
    x = np.broadcast_to(np.array([svals]).T, dims)
    summands = np.broadcast_to(np.array([summands]).T, dims)

    adder_mat = np.broadcast_to(np.arange(0, dims[0]), dims)

    return(x + adder_mat*summands)


def format_matrix_row(row):
    return("".join(["[" + str(i) + "]" for i in row]))


def format_digit_matrix(m, mask=None):

    mat_list = [list(i) for i in m]

    if mask is None:
        string_out = "\n".join([format_matrix_row(r) for r in mat_list])

    else:
        mat_list[-1] = mat_list[-1][:-1]
        string_out = "\n".join([format_matrix_row(r) for r in mat_list])
        string_out = string_out + mask

    return(string_out)    
```


```{python}
def make_np_matrix_constant(svals):
    # svals: numpy vector of starting values

    dims = (len(svals), len(svals))
    x = np.broadcast_to(np.array([svals]).T, dims)

    return(x)


def make_np_matrix_dist(svals):
    # svals: numpy vector of starting values

    dims = (len(svals), len(svals))

    x = np.array([
        svals,
        svals[[1, 2, 0]],
        svals[[2, 0, 1]]
        ])

    return(x)
```


```{python}
def mix_matrix_row(r1, r2):
    return("".join(["[" + str(r1[i]) + " " + str(r2[i]) + "]" for i in range(0, len(r1))]))

def mix_matrices(m1, m2, mask=None):

    m1_list = [list(i) for i in m1]
    m2_list = [list(i) for i in m2]

    if mask is None:
        string_out = "\n".join([mix_matrix_row(m1_list[i], m2_list[i]) for i in range(0, len(m1_list))])

    else:
        m1_list[-1] = m1_list[-1][:-1]
        m2_list[-1] = m2_list[-1][:-1]
        string_out = "\n".join([mix_matrix_row(m1_list[i], m2_list[i]) for i in range(0, len(m1_list))])
        string_out = string_out + mask

    return(string_out)

x1 = make_np_matrix_progression(np.array([11, 22, 33]), np.array([1, 2, 3]))
x2 = make_np_matrix_constant(np.array([41, 51, 61]))


print(mix_matrices(x1, x2))
```

I can make a lot of different matrices instantiating each of the individual relations while keeping the numbers in the realm of two-digits, and also while ensuring there are two distinct sets, an out of sample set for validation that has none of the same tokens as the created set.

Using the first 50 digits as "within sample" and the last 50 digits as "out of sample", there are in each group:

- constant: 50 choose 3 matrices that can be made, or 19600.
- distribution of three: 50 choose 3 matrices that can be made, or 19600.
- progression: 30 choose 3 = 4060 first vectors and 9 choose 3 = 84 summands, or ~341k matrices that can be made.
- two-rule: all combinations of these, so millions

In terms of relation-pairs, there are 6 different kinds:

1. constant - constant
2. distribution - distribution
3. progression - progression
4. constant - distribution
5. constant - progression
6. distribution - progression

Ignoring order (or keeping it fixed), we can have 6 different kinds of two-rule matrices. If we care about order, we can have 9 different kinds. And in terms of matrix pairs, we can have 6 choose 2 = 15 different if we don't care about order (or keep it fixed) and can have 9 choose 2 = 36 different if we do care about order.

If we extend to three-rule matrices, we can ahve 20 different kinds of three-rule matrices. In terms of matrix pairs, we can have 20 choose 2 = 190 different relation pairs if we don't care about order (or keep it fixed).

So going all the way to three-rule matrices gives us quite a few different kinds of matrices to work with, and to consider "out of sample". 

I think so long as we are focused on few-shot learning, then using 2-rule will be fine. We can give 3 "same" examples and 3 "different" examples and still have 3 and 6 oos examples to test on. And/or we can do a leave-out-one-relation cross-validation approach always holding one out.

```{python}

```



Looking ahead, I think that this syntax will be the best. Key is to make sure there is the leading space ahead of the important tokens. This lets a one-rule matrix for 23 tokens, and a 2-rule for 31 tokens.

```
[ 1][ 1][ 1]
[ 2][ 2][ 2]
[ 3][ 3][ 3]
```

Should be able to do 16-shot prompting even with 3-rule problems (~2560 tokens). But can't quite fit 32-shot with more than one rule. So will have to turn to fine-tuning to give more examples.


### Some tests on GPT-3

Looks like it's quite sensitive to the formatting of the input. The best approach I think will be to find a zero-shot syntax that works for the basic case of RMTS between words/tokens. Then I can use the same syntax for the more complex cases of RMTS between number matrices.


```
[ car ]
[ boat ]
----
[ boat ]
[ ? ]

? is:
A: [ boat ]
B: [ airplane ]

Answer:
```

```
< 
car
>
<
boat
>
----
<
boat
>
<
?
>

? is:
A:
<
boat
>
B
<
airplane
>

Answer: B: <airplane>
```

```
<
[ 1 ] [ 1 ] [ 1 ]
[ 2 ] [ 2 ] [ 2 ]
[ 3 ] [ 3 ] [ 3 ]
>
<
[ 6 ] [ 6 ] [ 6 ]
[ 3 ] [ 3 ] [ 3 ]
[ 1 ] [ 1 ] [ 1 ]
>
----
<
[ 5 ] [ 5 ] [ 5 ]
[ 2 ] [ 2 ] [ 2 ]
[ 0 ] [ 0 ] [ 0 ]
>
<
 ?
>

? is:
A:
<
[ 5 ] [ 2 ] [ 0 ]
[ 2 ] [ 0 ] [ 5 ]
[ 0 ] [ 5 ] [ 2 ]
>
B: 
<
[ 8 ] [ 8 ] [ 8 ]
[ 5 ] [ 5 ] [ 5 ]
[ 3 ] [ 3 ] [ 3 ]
>

Answer:
```

