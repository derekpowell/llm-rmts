---
title: "RMTS - tokens"
format: html
---

```{python}
import numpy as np
import pandas as pd
import openai
import configparser
import re 
import time
from  scipy.special import expit, logit

# import oai_helpers as oh
from oai_helpers import *

config = configparser.ConfigParser()
config.read("config.ini")
api_key = config.get('Keys','openai_api_key')

openai.api_key = api_key

```




## Relational Match-To-Sample

I'm going to try to use GPT-3 to learn relational match to sample. I'm going to start with the simplest case of matching two-digit numbers to two-digit numbers. Then I'll move on to matching arbitrary token sequences of varying length (again using two-digit numbers as the tokens)

### Creating RMTS dataset for testing

```{python}

### Make data
rng = np.random.default_rng(123)
N = 100

def make_trial_data(N, rng):
    df = pd.DataFrame({"type": ["same"]*int(N/2) + ["different"]*int(N/2), "p1": rng.integers(1, 51, size=N)})
    df["p2"] = df.apply(lambda x: x["p1"] if x["type"] == "same" else rng.integers(1, 51, size=1)[0], axis=1)
    df["m1"] = rng.integers(1, 51, size=N)
    df["correct"] = df.apply(lambda x: x["m1"] if x["type"] == "same" else rng.integers(1, 51, size=1)[0], axis=1)
    df["foil"] = df.apply(lambda x: x["m1"] if x["type"] == "different" else rng.choice([x["p1"], x["p2"]]), axis=1)

    ## now make sure correct and foil are not the same by changing the foil if needed
    df["foil"] = df.apply(lambda x: x["p1"] if x["foil"] == x["correct"] else x["foil"], axis=1)

    ## now add a new column prompts, randomizing whether "correct" or "foil" is the first or second option

    df["option1"] = df.apply(lambda x: x["correct"] if rng.choice([True, False]) else x["foil"], axis=1)
    df["option2"] = df.apply(lambda x: x["correct"] if x["option1"] == x["foil"] else x["foil"], axis=1)

    return(df)


df = make_trial_data(N, rng)
```


```{python}
## make data with characters instead of numbers

# df = pd.DataFrame({"type": ["same"]*50 + ["different"]*50, "p1": rng.choice(list("abcdefghijklmnopqrstuvwxyz"), size=N)})
# df["p2"] = df.apply(lambda x: x["p1"] if x["type"] == "same" else rng.choice(list("abcdefghijklmnopqrstuvwxyz"), size=1)[0], axis=1)
# df["m1"] = rng.choice(list("abcdefghijklmnopqrstuvwxyz"), size=N)
# df["correct"] = df.apply(lambda x: x["m1"] if x["type"] == "same" else rng.choice(list("abcdefghijklmnopqrstuvwxyz"), size=1)[0], axis=1)
# df["foil"] = df.apply(lambda x: x["m1"] if x["type"] == "different" else rng.choice([x["p1"], x["p2"]]), axis=1)


# ## define a function to return a new random character until the character returned does not match the input
# def get_new_char(x):
#     new_char = rng.choice(list("abcdefghijklmnopqrstuvwxyz"))
#     while new_char == x:
#         new_char = rng.choice(list("abcdefghijklmnopqrstuvwxyz"))
#     return(new_char)

# ## now make sure correct and foil are not the same by changing the foil if needed
# df["foil"] = df.apply(lambda x: get_new_char(x["correct"]) if x["foil"] == x["correct"] else x["foil"], axis=1)

# ## now add a new column prompts, randomizing whether "correct" or "foil" is the first or second option

# df["option1"] = df.apply(lambda x: x["correct"] if rng.choice([True, False]) else x["foil"], axis=1)
# df["option2"] = df.apply(lambda x: x["correct"] if x["option1"] == x["foil"] else x["foil"], axis=1)
# df["prompt"] = df.apply(lambda x: make_rmts_prompt(x["p1"], x["p2"], x["m1"], x["option1"], x["option2"]), axis=1)

# df["answer"] = df.apply(lambda x: "A" if x["option1"] == x["correct"] else "B", axis=1)
```


# Testing GPT-3

## Cloze vs multiple choice

This [paper](https://arxiv.org/pdf/2210.12353.pdf) proposes prompting with a standard multiple-choice style prompt and looking for the token GPT-3 responds with. The authors claim that this is a better way to evaluate GPT-3 than the cloze-style prompt.

A cloze-style prompt appends or embeds the answer and then examines the probability of the relevant tokens. There are various approaches to normalizing this. One that seems relevant would be the unconditional probability of the answer, just as a token by itself, or as in some other more minimal context, e.g. in the presence of "Answer: ". In my case it would be in the presence of "[" or maybe just the second row?

It seems worth exploring the cloze-style approach as well, since the zero-shot performance otherwise appears to be chance.

```{python}

def make_rmts_cloze_prompt(p1, p2, m1, completed):
    string = str(p1) + "\n.\n" + str(p2) + "\n---\n" + str(m1) + "\n.\n" + str(completed)

    return(string)    


def make_normalization_prompt(m1, completed):
    string = "\n---\n"+ str(m1) + "\n.\n" + str(completed)

    return(string)    


def make_cloze_prompts(df, option_name="correct"):
    prompts = [make_rmts_cloze_prompt(i[0], i[1],i[2], i[3]) for i in zip(df["p1"], df["p2"], df["m1"], df[option_name])]

    norm_prompts = [make_normalization_prompt(i[0], i[1]) for i in zip(df["m1"], df[option_name])]

    return(prompts, norm_prompts)


def count_cloze_prompt_tokens(df, option_name="correct", prompt=""):
    cloze_prompts, cloze_norm_prompts = make_cloze_prompts(df, option_name)
    x = [count_tokens(p) for p in cloze_prompts + cloze_norm_prompts]
    return(np.sum(x)+count_tokens(prompt)*len(cloze_prompts))


def compute_cloze_prob(df, option_name="correct", prompt="", ind = -2, model="text-davinci-003", sleep=0, **kwargs):
    cloze_prompts, cloze_norm_prompts = make_cloze_prompts(df, option_name)

    cloze_logprobs = gpt_token_probs(cloze_prompts, prompt=prompt, model=model, sleep=sleep, **kwargs)
    time.sleep(sleep)
    cloze_norm_logprobs = gpt_token_probs(cloze_norm_prompts, model=model, sleep=sleep, **kwargs) # may help to add prompt here too, unclear

    option_logprobs = np.array([r[ind] for r in cloze_logprobs])
    norm_logprobs = np.array([r[ind] for r in cloze_norm_logprobs])

    # return(option_logprobs)

    return(option_logprobs - norm_logprobs)
    # return(np.log(expit(option_logprobs)) - np.log(expit(norm_logprobs)))

```


```{python}
## compute a check!
print(count_cloze_prompt_tokens(df), "tokens")

```

```{python}

x = compute_cloze_prob(df, "correct", ind=-1, model = "code-davinci-001", sleep=5)
y = compute_cloze_prob(df, "foil", ind=-1, model = "code-davinci-001", sleep=5)

```


```{python}
df["gpt_correct_zeroshot"] = x > y
np.mean(df["gpt_correct_zeroshot"] )
```

Well it clearly matters a lot how you normalize the cloze probabilities. I don't think that the Brown et al approach makes much sense as a "normalization" -- the result is no longer a probability! What is working best so far is computing something like the "evidence" -- the difference in the logits for the full context and the minimal context. Without any normalization it tends to want to use the "same" response for everything, so it does nearly perfectly for the "same" items but does worse than chance for the "different" items. 


```{python}
df_examples = make_trial_data(32, np.random.default_rng(256))
examples_prompt_list, NULL = make_cloze_prompts(df_examples.sample(frac=1)[0:32], "correct")
examples_prompt = "\n\n".join(examples_prompt_list) + "\n\n"

print(count_cloze_prompt_tokens(df, prompt=examples_prompt)/100, "tokens each")
print(count_cloze_prompt_tokens(df, prompt=examples_prompt), "tokens total")
```

```{python}

x = compute_cloze_prob(df, "correct", prompt = examples_prompt, ind=-1, model = "text-davinci-003") 
y = compute_cloze_prob(df, "foil", prompt = examples_prompt, ind=-1, model = "text-davinci-003")

```


```{python}
df["gpt_correct_fewshot"] = x > y
np.mean(df["gpt_correct_fewshot"] )
```

Ok so multi-shot improves performance! Up from 74% to 82% for 16-shot. And up to 95% for 32-shot!

__comment__: It is somewhat sensitive to the format that things are presented in. The `[a][b]\n[c][d]` approach seems to be best for zero-shot. But my latest "vertical" approach also works and is similar I think. And this will work for the matrices too (at least in principle it should).


## To do

- Turn this into a something that can programmatically test zero, 2-shot, 4, 8, 16, 32 shot performance. Make a plot of performance vs. number of examples.
    - Separate lines for different models
- Formally test also for smaller models than davinci-003. Looks like scale is not the main issue. It fails zero-shot on anything other than davinci-003. But scale does matter, it gets to 65% accuracy with 32 shot examples on davinci-001, but still fails entirely on curie-001! So the human in-the-loop training is the real key so far.
- Test on codex models: code-davinci-001 (175B) and cushman-001 (12B)
- Modify so examples come from different set of tokens (50 - 99)

-----------

## Multiple-choice answers

```{python}
# q_prompt = 'Which would complete the pattern in the place of the question mark?'
q_prompt = ""

def enclose_item(item):
    return("[" + str(item) + "]")

# def make_rmts_prompt(p1, p2, m1, option1, option2):
#     string = enclose_item(p1) + "\n" + enclose_item(p1) + "\n" + enclose_item(p2) + "\n---\n" +  enclose_item(m1) + "\n" +  enclose_item(m1) + "\n" +  enclose_item("?") + "\n\n" + q_prompt + "\n" + "a: " + str(option1) + "\nb: " + str(option2) + "\n\nAnswer:"

#     return(string)

def make_mc_prompt(p1, p2, m1, option1, option2):
    string = enclose_item(p1) + " " + enclose_item(p2) + "\n" +  enclose_item(m1) + " " +  "?" + "\n" + q_prompt + "\n" + "a. " + enclose_item(option1) + "\nb. " + enclose_item(option2) + "\n\nAnswer:"

    return(string)

```

```{python}
df["prompt"] = df.apply(lambda x: make_mc_prompt(x["p1"], x["p2"], x["m1"], x["option1"], x["option2"]), axis=1)
## add a new column "answer" which is "A" if option1 is correct, "B" if option2 is correct
df["answer"] = df.apply(lambda x: "A" if x["option1"] == x["correct"] else "B", axis=1)

prompts = df["prompt"].tolist()

print(prompts[0])
```


```{python}
responses = gpt_complete(prompts, sleep=0, temperature = 0, max_tokens = 1, stop = [":", "]"])
```

```{python}
df["gpt_answer"] = responses
df["gpt_acc"] = df.apply(lambda x: x["answer"].lower() == x["gpt_answer"] or str(x["correct"])==x["gpt_answer"], axis=1)
print("acc:", df["gpt_acc"].mean())
```

Looks like it doesn't do well! Doesn't seem to do well with zero-shot relational match to sample task. Surprising because with my small anecdotal sample it seemed like it was doing very well


```{python}
example_same1 = make_rmts_prompt("2","2","3", "3", "2") + " a\n\n"
example_same2 = make_rmts_prompt("45","45","29", "45", "29") + " b\n\n"

example_diff1 = make_rmts_prompt("17","12","12", "12", "5") + " b\n\n"
example_diff2 = make_rmts_prompt("5","7","7", "2", "7") + " a\n\n"
```

```{python}

fewshot_prompt = example_diff1 + example_same1 + example_diff2 + example_same2

responses_1shot = gpt_complete(prompts, prompt = fewshot_prompt, sleep=0.1, temperature = 0, max_tokens = 12, stop = ["<", "\n"])

```

```{python}

def clean_response(x):
    return(x[0])


df["gpt_answer_1shot"] = [clean_response(r) for r in responses_1shot]
df["gpt_acc_1shot"] = df.apply(lambda x: x["answer"].lower() == x["gpt_answer_1shot"] or str(x["correct"])==x["gpt_answer_1shot"], axis=1)
print("acc (1shot):", df["gpt_acc_1shot"].mean())
```

Gets to similar performance as the best (still hacky) cloze-style approach with fewshot examples. Possibly it's partially struggling with the matching of answers to the options in the prompt, which the fewshot examples help to clear up. 
